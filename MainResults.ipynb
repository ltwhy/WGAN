{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Key code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "#The generator of WGAN\n",
    "generator = nn.Sequential(\n",
    "    nn.ConvTranspose2d(latent_dim, 512, kernel_size=3, stride=2),\n",
    "    nn.BatchNorm2d(512),\n",
    "    nn.ReLU(),\n",
    "    nn.ConvTranspose2d(512, 128, kernel_size=3, stride=2),\n",
    "    nn.BatchNorm2d(128),\n",
    "    nn.ReLU(),\n",
    "    nn.ConvTranspose2d(128, 64, kernel_size=2, stride=2),\n",
    "    nn.BatchNorm2d(64),\n",
    "    nn.ReLU(),\n",
    "    nn.ConvTranspose2d(64, channels, kernel_size=2, stride=2),\n",
    "    nn.Tanh()\n",
    ").to(device)\n",
    "\n",
    "#The generator of GAN\n",
    "generator2 = nn.Sequential(\n",
    "    # nn.ConvTranspose2d can be seen as the inverse operation\n",
    "    # of Conv2d, where after convolution we arrive at an\n",
    "    # upscaled image.\n",
    "    nn.ConvTranspose2d(latent_dim, 256, kernel_size=3, stride=2),\n",
    "    nn.BatchNorm2d(256),\n",
    "    nn.ReLU(),\n",
    "    nn.ConvTranspose2d(256, 128, kernel_size=3, stride=2),\n",
    "    nn.BatchNorm2d(128),\n",
    "    nn.ReLU(),\n",
    "    nn.ConvTranspose2d(128, 64, kernel_size=2, stride=2),\n",
    "    nn.BatchNorm2d(64),\n",
    "    nn.ReLU(),\n",
    "    nn.ConvTranspose2d(64, channels, kernel_size=2, stride=2),\n",
    "    nn.Sigmoid() # Image intensities are in [0, 1]\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "class Flatten(nn.Module):\n",
    "    def forward(self, x):\n",
    "        return x.view(x.size(0), -1)\n",
    "\n",
    "#The discriminator of WGAN\n",
    "discriminator = nn.Sequential(\n",
    "    nn.Conv2d(channels, 64, kernel_size=ks, stride=st, padding = pa),\n",
    "    nn.Dropout(p=0.5),\n",
    "    nn.LayerNorm(13),\n",
    "    nn.ReLU(),\n",
    "    nn.Conv2d(64, 128, kernel_size=3, stride=2, padding = 1),\n",
    "    nn.LayerNorm(7),\n",
    "    nn.ReLU(),\n",
    "    nn.Conv2d(128, 256, kernel_size=3, stride=2, padding = pa),\n",
    "    nn.LayerNorm(3),\n",
    "    nn.ReLU(),\n",
    "    nn.Conv2d(256, 512, kernel_size=3, stride=1, padding = pa),\n",
    "    nn.ReLU(),\n",
    "    Flatten(),\n",
    "    nn.Linear(512, 1),\n",
    ").to(device)\n",
    "\n",
    "#The discriminator of GAN\n",
    "discriminator2 = nn.Sequential(\n",
    "    nn.Conv2d(channels, 64, kernel_size=ks, stride=st, padding = pa),\n",
    "    nn.BatchNorm2d(64),\n",
    "    nn.LeakyReLU(0.2),\n",
    "    nn.Conv2d(64, 128, kernel_size=ks, stride=st, padding = pa),\n",
    "    nn.BatchNorm2d(128),\n",
    "    nn.LeakyReLU(0.2),\n",
    "    nn.Conv2d(128, 512, kernel_size=ks, stride=st, padding = pa),\n",
    "    nn.BatchNorm2d(512),\n",
    "    nn.LeakyReLU(0.2),\n",
    "    Flatten(),\n",
    "    nn.Linear(512, 1),\n",
    "    nn.Sigmoid()\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "#spectral normalization\n",
    "class SpectralNorm(nn.Module):\n",
    "    def __init__(self, module, name='weight', power_iterations=1):\n",
    "        super(SpectralNorm, self).__init__()\n",
    "        self.module = module\n",
    "        self.name = name\n",
    "        self.power_iterations = power_iterations\n",
    "        if not self._made_params():\n",
    "            self._make_params()\n",
    "\n",
    "    def _update_u_v(self):\n",
    "        u = getattr(self.module, self.name + \"_u\")\n",
    "        v = getattr(self.module, self.name + \"_v\")\n",
    "        w = getattr(self.module, self.name + \"_bar\")\n",
    "\n",
    "        height = w.data.shape[0]\n",
    "        for _ in range(self.power_iterations):\n",
    "            v.data = l2normalize(torch.mv(torch.t(w.view(height,-1).data), u.data))\n",
    "            u.data = l2normalize(torch.mv(w.view(height,-1).data, v.data))\n",
    "\n",
    "        # sigma = torch.dot(u.data, torch.mv(w.view(height,-1).data, v.data))\n",
    "        sigma = u.dot(w.view(height, -1).mv(v))\n",
    "        setattr(self.module, self.name, w / sigma.expand_as(w))\n",
    "\n",
    "    def _made_params(self):\n",
    "        try:\n",
    "            u = getattr(self.module, self.name + \"_u\")\n",
    "            v = getattr(self.module, self.name + \"_v\")\n",
    "            w = getattr(self.module, self.name + \"_bar\")\n",
    "            return True\n",
    "        except AttributeError:\n",
    "            return False\n",
    "\n",
    "\n",
    "    def _make_params(self):\n",
    "        w = getattr(self.module, self.name)\n",
    "\n",
    "        height = w.data.shape[0]\n",
    "        width = w.view(height, -1).data.shape[1]\n",
    "\n",
    "        u = Parameter(w.data.new(height).normal_(0, 1), requires_grad=False)\n",
    "        v = Parameter(w.data.new(width).normal_(0, 1), requires_grad=False)\n",
    "        u.data = l2normalize(u.data)\n",
    "        v.data = l2normalize(v.data)\n",
    "        w_bar = Parameter(w.data)\n",
    "\n",
    "        del self.module._parameters[self.name]\n",
    "\n",
    "        self.module.register_parameter(self.name + \"_u\", u)\n",
    "        self.module.register_parameter(self.name + \"_v\", v)\n",
    "        self.module.register_parameter(self.name + \"_bar\", w_bar)\n",
    "\n",
    "\n",
    "    def forward(self, *args):\n",
    "        self._update_u_v()\n",
    "        return self.module.forward(*args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "#Training\n",
    "for epoch in range(num_epochs):\n",
    "        batch_d_loss, batch_g_loss = [], []\n",
    "        dt = torch.zeros((2,batch_size,channels,image_size,image_size))\n",
    "        k = 0\n",
    "    \n",
    "        for x, _ in train_loader:\n",
    "            if x.size(0) == batch_size:\n",
    "                # True data is given label 1, while fake data is given label 0\n",
    "                true_label = torch.ones(batch_size, 1).to(device)\n",
    "                fake_label = torch.zeros(batch_size, 1).to(device)\n",
    "                \n",
    "                if LOSS == 'BCEWLS':\n",
    "                    discriminator2.zero_grad()\n",
    "                    generator2.zero_grad()\n",
    "                    \n",
    "                    # Step 1. Send real data through discriminator\n",
    "                    #         and backpropagate its errors.\n",
    "                    x_true = Variable(x).to(device)        \n",
    "                    output = discriminator2(x_true)\n",
    "                    \n",
    "                    error_true = loss(output, true_label)                    \n",
    "                    error_true.backward()\n",
    "                    \n",
    "                    # Step 2. Generate fake data G(z), where z ~ N(0, 1)\n",
    "                    #         is a latent code.\n",
    "                    z = torch.randn(batch_size, latent_dim, 1, 1)\n",
    "                    z = Variable(z, requires_grad=False).to(device)\n",
    "                    \n",
    "                    x_fake = generator2(z)\n",
    "                        \n",
    "                    # Step 3. Send fake data through discriminator\n",
    "                    #         propagate error and update D weights.\n",
    "                    # --------------------------------------------\n",
    "                    # Note: detach() is used to avoid compounding generator gradients\n",
    "                    output = discriminator2(x_fake.detach()) \n",
    "                    error_fake = loss(output, fake_label)                  \n",
    "                    error_fake.backward()\n",
    "                    discriminator_optim2.step()\n",
    "                    \n",
    "                    # Step 4. Send fake data through discriminator _again_\n",
    "                    #         propagate the error of the generator and\n",
    "                    #         update G weights.\n",
    "                    output = discriminator2(x_fake)\n",
    "                    error_generator = loss(output, true_label)              \n",
    "                    error_generator.backward()\n",
    "                    generator_optim2.step()\n",
    "                    \n",
    "                    batch_d_loss.append((error_true/(error_true + error_fake)).item())\n",
    "                    batch_g_loss.append(error_generator.item())\n",
    "                    batches_done = epoch * len(train_loader) + x.size(0)\n",
    "                    \n",
    "                if LOSS == 'wasserstein':\n",
    "                    \n",
    "                   \n",
    "                    discriminator.zero_grad()              \n",
    "                    generator.zero_grad()\n",
    "                    \n",
    "                    z = torch.randn(batch_size, latent_dim, 1, 1)\n",
    "                    z= Variable(z, requires_grad=False).to(device)  \n",
    "\n",
    "                    x_true = Variable(x).to(device) \n",
    "                    x_fake = generator(z)   \n",
    "\n",
    "                    error_true = -discriminator(x_true).mean()+discriminator(x_fake).mean() \n",
    "                    error_true.backward()\n",
    "                    \n",
    "\n",
    "                    discriminator_optim.step()  \n",
    "\n",
    "                    for i in range(disc_iters):\n",
    "                        discriminator.zero_grad()\n",
    "                        generator.zero_grad()\n",
    "                        z = torch.randn(batch_size, latent_dim, 1, 1)\n",
    "                        z = Variable(z, requires_grad=False).to(device)                \n",
    "                        x_fake = generator(z)\n",
    "                        error_fake = - discriminator(generator(z)).mean()\n",
    "                        error_fake.backward()\n",
    "                        generator_optim.step()\n",
    "                    \n",
    "                    \n",
    "                    batch_d_loss.append(error_true.item())\n",
    "                    batch_g_loss.append(error_fake.item())\n",
    "                    batches_done = epoch * len(train_loader) + x.size(0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.Experimental results\n",
    "### 2.1 Results on MNIST\n",
    "\n",
    "Real images of MNIST:\n",
    "\n",
    "<img src=\"images/true_mnist.png\" />\n",
    "\n",
    "Loss and fake images of the standard GAN:\n",
    "\n",
    "<img src=\"images/fake_mnist_gan.png\" />\n",
    "\n",
    "\n",
    "Loss and fake images of the WGAN with gradient clipping:\n",
    "\n",
    "<img src=\"images/fake_mnist_gc.png\" />\n",
    "\n",
    "Loss and fake images of the WGAN with spectral normalization:\n",
    "\n",
    "<img src=\"images/fake_mnist_sn.png\" />\n",
    "\n",
    "### 2.2 Results on CIFAR10\n",
    "\n",
    "Real images of CIFAR10:\n",
    "\n",
    "<img src=\"images/true_cifar.png\" />\n",
    "\n",
    "Fake images of the standard GAN:\n",
    "\n",
    "<img src=\"images/fake_cifar_gan.png\" />\n",
    "\n",
    "\n",
    "Fake images of the WGAN with gradient clipping:\n",
    "\n",
    "<img src=\"images/fake_cifar_gc.png\" />\n",
    "\n",
    "Fake images of the WGAN with spectral normalization:\n",
    "\n",
    "<img src=\"images/fake_cifar_sn.png\" />\n",
    "\n",
    "### 2.3 Quantitative results\n",
    "\n",
    "Fid score:\n",
    "\n",
    "<img src=\"images/fid_bar.png\" />"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
